<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <style>
  img {
     display: block;
     margin-left: auto;
     margin-right: auto;
      }
  </style>
  <title>ML-CSS@ICL - Javier Carnerero Cano</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Krub:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Bikin - v4.3.0
  * Template URL: https://bootstrapmade.com/bikin-free-simple-landing-page-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center justify-content-between">

      <h1 class="logo"><a href="index.html">ML-CSS@ICL</a></h1> 
      <!-- Uncomment below if you prefer to use an image logo -->
      <!-- <a href="index.html" class="logo"><img src="assets/img/mlcss.png" alt="" class="img-fluid"></a> -->

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto" href="index.html#hero">Home</a></li>
          <li><a class="nav-link scrollto " href="index.html#details">Details</a></li>
          <li><a class="nav-link scrollto" href="index.html#speakers">Keynote Speakers</a></li>          
          <li><a class="nav-link scrollto" href="index.html#schedule">Schedule</a></li>
          <li><a class="nav-link scrollto" href="index.html#organisers">Organisers</a></li> 
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero" class="align-items-center">
    <div class="container d-flex flex-column align-items-center justify-content-center"  style="padding-bottom: 2%">
      <a href="https://javiccano.github.io/"><img src="assets/img/javier.png" class="img-fluid" alt="Javier Carnerero Cano" width=200></a>
      <br/> <br/>
      <a href="https://javiccano.github.io/"><h3>Javier Carnerero Cano</h3></a>
      <p class="fst-italic">
         Ph.D. Student in Computing
      </p>
      <p>Javier Carnerero Cano is a final-year PhD student in the Resilient Information Systems Security (RISS) Group in the Department of Computing at Imperial College London. His current interests are adversarial machine learning and its intersections with bilevel optimisation, Generative Adversarial Networks (GANs), and federated learning. He focuses on data poisoning attacks, where attackers can manipulate training data collected from untrusted sources to subvert learning and degrade the machine learning model's performance. In his PhD project, he actively collaborates with the Defence Science and Technology Laboratory (Dstl). He did a research internship in summer 2022 at IBM Research on machine learning security and machine unlearning. His background is also in telecommunications engineering, electromagnetic sensors and antennas.

      </p>
      <br/><br/>
      <p><i>Scroll down for more details...</i></p>

    </div>
  </section><!-- End Hero -->

  <main id="main">

    <section id="talks" class="services">
      <div class="container" data-aos="fade-up">
        <div class="section-title">
          <br/><br/>
          <h2>Fantastic Data Poisoning Attacks, Hyperparameters and Where to Find Them</h2>
          <p>Lightning Talk</p>
           <br/>
           <i>Machine Learning Security; Poisoning Attacks</i>
        </div>
      </div>
    </section><!-- End Services Section -->
    
    <section id="prof" class="features" data-aos="fade-up">
      <div class="container">
        <div class="section-title">
          <p>
In many situations, machine learning systems are trained on data collected from untrusted sources, such as humans, sensors, the Internet, or IoT devices, which can be compromised and manipulated. These scenarios expose machine learning models to data poisoning attacks, where attackers can manipulate a fraction of the training data to subvert the learning process, e.g., to increase the error or bias of the model. <br>
In this talk, I will give an overview of data poisoning attacks and, in particular, optimal poisoning attacks, which are useful to evaluate the robustness of machine learning models in worst-case scenarios. In this regard, some of the previous attacks target models that have hyperparameters, but the hyperparameters are considered constant regardless of the fraction of poisoning points injected in the training dataset. This can provide a misleading analysis of the robustness of the algorithms against such attacks, as the value of the hyperparameters can change depending on the type and strength of the attack. <br>
To overcome this, I will describe a novel formulation that considers the effect of the attack on the hyperparameters of the model, and, for the case of L2 regularisation, I will provide empirical evidence that learning the correct hyperparameters can help protect machine learning models against these attacks. This allows to formulate optimal attacks, learn hyperparameters and evaluate robustness under worst-case conditions. We apply this attack formulation to logistic regression and neural network classifiers. Our evaluation on multiple datasets shows that choosing an “a priori” constant value for the regularization hyperparameter can be detrimental to the performance of the algorithms. This confirms the limitations of previous strategies and evidences the benefits of using L2 regularization to dampen the effect of poisoning attacks, when hyperparameters are learned using a small trusted dataset.          </p>

        </div>
      </div>
    
    </section><!-- End Scholarship Section -->
      
    <!-- ======= Services Section ======= -->
    <section id="footer" class="services">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          <br/><br/><br/>
          <h2>See you there <img src="assets/img/smile.png" class="img-fluid" alt=":)" width=50></h2>
        </div>

      </div>
    </section><!-- End Services Section -->

  </main><!-- End #main -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
